{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. You are work#ng on a mach#ne learn#ng project where you have a dataset conta#n#ng numer#cal and\n",
        "categor#cal features. You have #dent#f#ed that some of the features are h#ghly correlated and there are\n",
        "m#ss#ng values #n some of the columns. You want to bu#ld a p#pel#ne that automates the feature\n",
        "eng#neer#ng process and handles the m#ss#ng valuesD\n",
        "Des#gn a p#pel#ne that #ncludes the follow#ng steps\"\n",
        "Use an automated feature select#on method to #dent#fy the #mportant features #n the datasetC\n",
        "Create a numer#cal p#pel#ne that #ncludes the follow#ng steps\"\n",
        "Impute the m#ss#ng values #n the numer#cal columns us#ng the mean of the column valuesC\n",
        "Scale the numer#cal columns us#ng standard#sat#onC\n",
        "Create a categor#cal p#pel#ne that #ncludes the follow#ng steps\"\n",
        "Impute the m#ss#ng values #n the categor#cal columns us#ng the most frequent value of the columnC\n",
        "One-hot encode the categor#cal columnsC\n",
        "Comb#ne the numer#cal and categor#cal p#pel#nes us#ng a ColumnTransformerC\n",
        "Use a Random Forest Class#f#er to bu#ld the f#nal modelC\n",
        "Evaluate the accuracy of the model on the test datasetD\n",
        "Note! Your solut#on should #nclude code sn#ppets for each step of the p#pel#ne, and a br#ef explanat#on of\n",
        "each step. You should also prov#de an #nterpretat#on of the results and suggest poss#ble #mprovements for\n",
        "the p#pel#neD"
      ],
      "metadata": {
        "id": "mUmavbAwGXqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, I can provide you with a Python code snippet that demonstrates how to build a machine learning pipeline that automates feature engineering, handles missing values, and uses a Random Forest Classifier as the final model. This example will walk you through the steps and include code snippets for each part of the pipeline. Let's break it down:\n",
        "\n",
        "1. **Automated Feature Selection**:\n",
        "   - Use an automated feature selection method like Recursive Feature Elimination (RFE) with a Random Forest Classifier to identify important features in the dataset. The code snippet below demonstrates how to perform feature selection:\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the RFE feature selector\n",
        "feature_selector = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=10)\n",
        "\n",
        "# Fit the feature selector on your training data\n",
        "selected_features = feature_selector.fit(X_train, y_train)\n",
        "\n",
        "# Use the selected features for both training and test data\n",
        "X_train_selected = selected_features.transform(X_train)\n",
        "X_test_selected = selected_features.transform(X_test)\n",
        "```\n",
        "\n",
        "2. **Numerical Pipeline**:\n",
        "   - Create a numerical pipeline to impute missing values using the mean of the column values and scale the numerical columns using standardization.\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define the numerical pipeline\n",
        "numerical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Fit and transform the training data using the numerical pipeline\n",
        "X_train_numerical = numerical_pipeline.fit_transform(X_train)\n",
        "X_test_numerical = numerical_pipeline.transform(X_test)\n",
        "```\n",
        "\n",
        "3. **Categorical Pipeline**:\n",
        "   - Create a categorical pipeline to impute missing values in categorical columns using the most frequent value and perform one-hot encoding.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Define the categorical pipeline\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Fit and transform the training data using the categorical pipeline\n",
        "X_train_categorical = categorical_pipeline.fit_transform(X_train_categorical)\n",
        "X_test_categorical = categorical_pipeline.transform(X_test_categorical)\n",
        "```\n",
        "\n",
        "4. **ColumnTransformer**:\n",
        "   - Use `ColumnTransformer` to combine the numerical and categorical pipelines into a single feature matrix.\n",
        "\n",
        "```python\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Specify which columns are numerical and which are categorical\n",
        "numerical_features = [...]  # List of column names or indices\n",
        "categorical_features = [...]  # List of column names or indices\n",
        "\n",
        "# Create the ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ])\n",
        "```\n",
        "\n",
        "5. **Random Forest Classifier**:\n",
        "   - Build a Random Forest Classifier as the final model using the preprocessed data.\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create the Random Forest Classifier model\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model to the preprocessed training data\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "```\n",
        "\n",
        "6. **Evaluation**:\n",
        "   - Evaluate the accuracy of the model on the test dataset and print the results.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Calculate the accuracy on the test dataset\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on the test dataset:\", accuracy)\n",
        "```\n",
        "\n",
        "**Interpretation**:\n",
        "The pipeline automates feature selection, handles missing values, preprocesses numerical and categorical features separately, and builds a Random Forest Classifier. The final model's accuracy is evaluated on the test dataset.\n",
        "\n",
        "**Possible Improvements**:\n",
        "1. You can further tune hyperparameters of the Random Forest Classifier to optimize model performance.\n",
        "2. Consider trying different feature selection techniques to identify the most important features.\n",
        "3. Experiment with other imputation strategies for missing values (e.g., median imputation, machine learning-based imputation).\n",
        "4. Explore different preprocessing techniques for categorical data (e.g., label encoding, target encoding) and choose the one that suits your dataset and problem.\n",
        "\n",
        "Remember that this is a basic example, and for a real project, extensive data preprocessing, hyperparameter tuning, and additional model evaluation techniques would be necessary."
      ],
      "metadata": {
        "id": "sqQmBnS9GdMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Bu#ld a p#pel#ne that #ncludes a random forest class#f#er and a log#st#c regress#on class#f#er, and then\n",
        "use a vot#ng class#f#er to comb#ne the#r pred#ct#ons. Tra#n the p#pel#ne on the #r#s dataset and evaluate #ts\n",
        "accuracy."
      ],
      "metadata": {
        "id": "PBJjkCTLGpTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build a pipeline that includes both a Random Forest Classifier and a Logistic Regression Classifier, and then combines their predictions using a Voting Classifier, you can use the following code snippet. We'll use the Iris dataset as an example for demonstration. Make sure you have the necessary libraries (scikit-learn) installed.\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual classifiers\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr_classifier = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Create a Voting Classifier that combines the two classifiers\n",
        "voting_classifier = VotingClassifier(estimators=[('rf', rf_classifier), ('lr', lr_classifier)], voting='hard')\n",
        "\n",
        "# Train the ensemble model on the training data\n",
        "voting_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = voting_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of the Voting Classifier:\", accuracy)\n",
        "```\n",
        "\n",
        "In this code:\n",
        "\n",
        "- We load the Iris dataset and split it into training and test sets.\n",
        "- We create two individual classifiers: a Random Forest Classifier (`rf_classifier`) and a Logistic Regression Classifier (`lr_classifier`).\n",
        "- We create a Voting Classifier (`voting_classifier`) that combines the predictions of both classifiers using majority voting (voting='hard').\n",
        "- We train the ensemble model on the training data and evaluate its accuracy on the test data.\n",
        "\n",
        "You can adapt this code to your specific dataset and classification task by replacing the dataset and classifier configurations. The Voting Classifier allows you to leverage the strengths of different algorithms for improved classification performance."
      ],
      "metadata": {
        "id": "orDiA6dcGs5j"
      }
    }
  ]
}